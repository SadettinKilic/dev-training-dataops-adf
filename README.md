<p align="left">
  <strong>Language:</strong>
  <img src="https://img.shields.io/badge/T%C3%BCrk%C3%A7e-red">
  <a href="./README_EN.md">
    <img src="https://img.shields.io/badge/English-lightgrey">
  </a>
</p>
<p align="left">
  <!-- Core Tech -->
  <img src="https://img.shields.io/badge/Azure-Data%20Factory-blue">
  <img src="https://img.shields.io/badge/Azure-Databricks-tomato">
  <img src="https://img.shields.io/badge/Microsoft%20Azure-Cloud-blue">

  <!-- Architecture -->
  <img src="https://img.shields.io/badge/Architecture-Medallion-success">

  <!-- REAL CI BADGE -->
  <img src="https://img.shields.io/badge/CI%2FCD-Handled%20in%20dbt%20Repo-lightgrey">
  <!-- DataOps -->
  <img src="https://img.shields.io/badge/DataOps-Automated-informational">
</p>

* * *

# ğŸ… Paris 2024 Olympics DataOps & Analytics Project

Bu proje, Paris 2024 Yaz Olimpiyat OyunlarÄ± verilerini kullanarak **End-to-End (UÃ§tan Uca)** bir Data Engineering pipeline'Ä± oluÅŸturmayÄ± hedefler. Azure ekosistemi Ã¼zerinde **Medallion Architecture** prensiplerine uygun olarak tasarlanmÄ±ÅŸ; ADF, Databricks ve dbt teknolojilerini bir araya getiren modern bir veri platformudur.
* * *
## ğŸ—ï¸ Mimari ve Kapsam
Proje, verinin ham halinden (CSV/Parquet) raporlanabilir AltÄ±n (Gold) tablolar haline gelene kadar geÃ§tiÄŸi tÃ¼m sÃ¼reÃ§leri kapsar.
### **Teknoloji Stack'i**
-   **Orkestrasyon:** Azure Data Factory (ADF)
-   **Veri AmbarÄ± & Ä°ÅŸleme:** Azure Databricks (Spark & Serverless SQL Warehouse)
-   **Transformasyon:** dbt (data build tool)
-   **Veri GÃ¶lÃ¼:** Azure Data Lake Storage Gen2 (ADLS Gen2)
-   **GÃ¼venlik:** Azure Key Vault & Managed Identity (RBAC)
-   **SÃ¼rÃ¼m KontrolÃ¼:** GitHub (ADF & dbt integration)
### ğŸ“‚ Proje DepolarÄ± (Repositories)

Proje, DataOps prensipleri gereÄŸi transformasyon kodlarÄ± ve orkestrasyon yapÄ±landÄ±rmasÄ± olarak iki ayrÄ± depoda yÃ¶netilmektedir:

| **Repository** | **Ä°Ã§erik** | **Link** |
| --- | --- | --- |
| **dbt Repository** | dbt Modelleri, SQL logic, CI/CD (SQLFluff, Freshness) | [ğŸ”— dev-training-dataops](https://github.com/SadettinKilic/dev-training-dataops) |
| **ADF Repository** | Azure Data Factory Pipeline'larÄ±, JSON tanÄ±mlarÄ±, Linked Services, Triggers | [ğŸ”— dev-training-dataops-adf](https://github.com/SadettinKilic/dev-training-dataops-adf) |
* * *
## ğŸ“‚ Azure Kaynak YapÄ±sÄ± (Resource Group: `rg-training-dataops`)

| **Kaynak AdÄ±** | **TÃ¼rÃ¼** | **GÃ¶revi** |
| --- | --- | --- |
| `sttrainingdataops` | Storage Account | Bronze, Silver, Gold katmanlarÄ±nÄ± barÄ±ndÄ±ran veri gÃ¶lÃ¼. |
| `kv-training-dataops` | Key Vault | Token ve baÄŸlantÄ± bilgilerinin gÃ¼venli depolanmasÄ±. |
| `dev-training-dataops-adf` | Data Factory | Pipeline'larÄ±n yÃ¶netimi ve zamanlanmasÄ±. |
| `dbx-training-dataops-dev` | Databricks | dbt modellerinin Ã§alÄ±ÅŸtÄ±rÄ±ldÄ±ÄŸÄ± ana iÅŸlem motoru. |
| `dbx-connector-training-dataops-dev` | Access Connector | Databricks'in Storage'a eriÅŸimi iÃ§in yÃ¶netilen kimlik. |

* * *

## ğŸš€ Kurulum ve Uygulama AdÄ±mlarÄ±

Bu projeyi sÄ±fÄ±rdan ayaÄŸa kaldÄ±rmak iÃ§in aÅŸaÄŸÄ±daki adÄ±mlarÄ± izleyin:

### 1\. AltyapÄ± ve Veri KatmanlarÄ±nÄ±n HazÄ±rlanmasÄ±

Storage Account Ã¼zerinde aÅŸaÄŸÄ±daki container yapÄ±sÄ±nÄ± oluÅŸturun:
-   `source/raw_data`: Kaggle'dan indirilen ham CSV dosyalarÄ±.
-   `bronze`, `silver`, `gold`: Ä°ÅŸlenmiÅŸ veri katmanlarÄ±.
-   `dbx-managed`: Databricks Managed Catalog iÃ§in ayrÄ±lmÄ±ÅŸ alan.
-   `monitoring`: monitoring tablolarÄ±nÄ± iÃ§eriyor. Dashboard iÃ§in kullanÄ±lacak.

### 2\. IAM ve GÃ¼venlik YapÄ±landÄ±rmasÄ± (Ã–nemli)

Azure Ã¼zerinde servislerin birbiriyle konuÅŸabilmesi iÃ§in ÅŸu yetkileri tanÄ±mlayÄ±n:
-   **Storage Account:** Databricks Access Connector'a `Storage Blob Data Contributor` yetkisi verin.
-   **Key Vault:** ADF'e ÅŸifreleri okuyabilmesi iÃ§in `Key Vault Secrets User` yetkisi verin. Kendi kullanÄ±cÄ±nÄ±za ise `Key Vault Administrator` yetkisi tanÄ±mlayÄ±n.
-   **RBAC:** TÃ¼m kaynaklarÄ± tek bir Resource Group altÄ±nda toplayarak eriÅŸim yÃ¶netimini merkezileÅŸtirin.
    

### 3\. Databricks Katalog ve Åema Kurulumu

Databricks SQL Editor Ã¼zerinden Unity Catalog yapÄ±sÄ±nÄ± kurun (Ã–ncesinde external locationlarÄ± oluÅŸturmanÄ±z gerekecektir):
```sql
    CREATE CATALOG IF NOT EXISTS dataops MANAGED LOCATION 'abfss://dbx-managed@sttrainingdataops.dfs.core.windows.net/';
    USE CATALOG dataops; 
    
    CREATE SCHEMA IF NOT EXISTS bronze MANAGED LOCATION 'abfss://bronze@sttrainingdataops.dfs.core.windows.net/';
    CREATE SCHEMA IF NOT EXISTS silver MANAGED LOCATION 'abfss://silver@sttrainingdataops.dfs.core.windows.net/';
    CREATE SCHEMA IF NOT EXISTS gold MANAGED LOCATION 'abfss://gold@sttrainingdataops.dfs.core.windows.net/';
    CREATE SCHEMA IF NOT EXISTS monitoring MANAGED LOCATION 'abfss://monitoring@sttrainingdataops.dfs.core.windows.net/';
```
### 4. Veri TanÄ±mlama ve Bronze Tablo YapÄ±larÄ±

###   
Bronze katmanÄ±ndaki tablolar, ham verilerin (CSV/Parquet) ÅŸemalarÄ±nÄ± koruyarak Unity Catalog altÄ±nda ÅŸu ÅŸekilde tanÄ±mlanmÄ±ÅŸtÄ±r:

### 
```sql
    /* Katalog ve Åema BaÄŸlamÄ±nÄ± Ayarla */
    USE CATALOG dataops;
    USE SCHEMA bronze;
    
    /* Sporcu Tablosu (Parquet FormatÄ±) */
    CREATE TABLE IF NOT EXISTS bronze.raw_athletes
    USING PARQUET
    LOCATION 'abfss://bronze@sttrainingdataops.dfs.core.windows.net/athletes/';
    
    /* AntrenÃ¶r Tablosu (Parquet FormatÄ±) */
    CREATE TABLE IF NOT EXISTS bronze.raw_coaches
    USING PARQUET
    LOCATION 'abfss://bronze@sttrainingdataops.dfs.core.windows.net/coaches/';
    
    /* Etkinlik Tablosu (Parquet FormatÄ±) */
    CREATE TABLE IF NOT EXISTS bronze.raw_events
    USING PARQUET
    LOCATION 'abfss://bronze@sttrainingdataops.dfs.core.windows.net/events/';
    
    /* NOC (Milli Olimpiyat Komiteleri) Tablosu (CSV FormatÄ±) */
    CREATE TABLE IF NOT EXISTS bronze.raw_nocs
    USING CSV
    OPTIONS (header='true', inferSchema='true')
    LOCATION 'abfss://bronze@sttrainingdataops.dfs.core.windows.net/nocs/';

    CREATE TABLE IF NOT EXISTS dataops.monitoring.audit_logs (
      model_name STRING,
      execution_time TIMESTAMP,
      row_count LONG,
      status STRING
    ) USING DELTA;
```
### 5\. ADF Pipeline YapÄ±landÄ±rmasÄ±
ADF Ã¼zerinde iki ana sÃ¼reÃ§ yÃ¶netilmektedir:
-   **Ingestion:** `raw_to_bronze` pipeline'Ä±, `bronze/param.json` dosyasÄ±ndaki metadata'yÄ± okuyarak ham verileri dinamik olarak Bronze katmanÄ±na taÅŸÄ±r.
-   **Transformation:** `dbt_dataops_gold_daily` pipeline'Ä±, Key Vault Ã¼zerinden aldÄ±ÄŸÄ± token ile Databricks API'sini tetikler ve dbt modellerini koÅŸturur.  
### 6\. dbt (Data Build Tool) Entegrasyonu
`dev-training-dataops` reposundaki dbt projesini Databricks'e baÄŸlayÄ±n:
-   `profiles.yml` dosyasÄ±nda Databricks host ve http\_path bilgilerini tanÄ±mlayÄ±n.
-   `dbt_project.yml` iÃ§inde modelleri katmanlara gÃ¶re organize edin.
-   `CI/CD:` GitHub Actions kullanarak kod her push edildiÄŸinde dbt testlerinin Ã§alÄ±ÅŸmasÄ±nÄ± saÄŸlayÄ±n.  

### 7\. CI/CD SÃ¼reci (GitHub Actions)

Projenin dbt tarafÄ±, kod kalitesini ve sÃ¼rekliliÄŸi saÄŸlamak iÃ§in GitHub Actions ile entegre edilmiÅŸtir. Bu sayede manuel hatalarÄ±n Ã¶nÃ¼ne geÃ§ilir ve kod her zaman "Ã§alÄ±ÅŸmaya hazÄ±r" durumda tutulur.  
  
**1\. Otomatik Denetimler ve Veri Kalitesi**
-   **SQLFluff (Linting):** Projeye dahil edilen tÃ¼m SQL kodlarÄ± otomatik olarak taranÄ±r. Belirlenen yazÄ±m standartlarÄ±na (girintiler, bÃ¼yÃ¼k harf kullanÄ±mÄ± vb.) uymayan kodlar tespit edilerek dÃ¼zeltilir. Github Actions'Ä±n repository Ã¼stÃ¼nde yazma/okuma yetkisi vardÄ±r. KodlarÄ± otomatik olarak dÃ¼zenler/dÃ¼zenleyemediÄŸi satÄ±rlarÄ± da logda belirtir.
-   **dbt Freshness:** `models/silver` altÄ±nda yer alan `sources.yml` dosyasÄ±ndaki freshness konfigÃ¼rasyonunu okuyarak, Raw (Bronze) verinin gÃ¼ncel olup olmadÄ±ÄŸÄ±nÄ± denetler. Belirlenen sÃ¼reden eski veri varsa pipeline uyarÄ± verir.  

**2\. Deployment Pipeline**
    

GitHub Ã¼zerindeki workflow dosyamÄ±z ÅŸu adÄ±mlarÄ± otomatik olarak gerÃ§ekleÅŸtirir:

-   **Ortam Kurulumu:** Gerekli Python kÃ¼tÃ¼phaneleri ve `dbt-databricks` adaptÃ¶rÃ¼ yÃ¼klenir.
-   **BaÄŸlantÄ± Testi:** Databricks SQL Warehouse baÄŸlantÄ±sÄ± doÄŸrulanÄ±r.
-   **Kod Testi:** dbt modelleri Ã¼zerinde temel testler koÅŸturularak logic hatalarÄ± kontrol edilir.  
      
**3\. SÃ¼rÃ¼m KontrolÃ¼ ve Entegrasyon**
    
-   **ADF & dbt Sync:** ADF Ã¼zerindeki Web Activity, her zaman GitHub'daki "Production" branch'inde bulunan en gÃ¼ncel dbt kodunu tetikler. BÃ¶ylece geliÅŸtirme (dev) ortamÄ±nda yapÄ±lan testler onaylanmadan canlÄ±ya geÃ§mez.

### ğŸ“Š Ä°zleme ve GÃ¶zlemlenebilirlik (DataOps Dashboard)
### 
Projenin saÄŸlÄ±ÄŸÄ±, performansÄ± ve veri kalitesi **Databricks SQL Dashboard** Ã¼zerinden anlÄ±k olarak takip edilmektedir.
-   **Pipeline GÃ¼venilirliÄŸi:** GÃ¼nlÃ¼k baÅŸarÄ±lÄ±/hatalÄ± model Ã§alÄ±ÅŸmalarÄ±.  
-   **Model PerformansÄ± (Wall of Shame):** En Ã§ok kaynak tÃ¼keten ve optimizasyon gerektiren modellerin tespiti.
-   **Veri AkÄ±ÅŸ HÄ±zÄ± (Throughput):** Saniyede iÅŸlenen satÄ±r sayÄ±sÄ± bazÄ±nda SQL verimlilik analizi.
-   **Veri Hacmi Drift Analizi:** Kaynak sistemlerden gelen veri miktarÄ±ndaki ani deÄŸiÅŸimlerin takibi.

### ğŸ“– CanlÄ± DÃ¶kÃ¼mantasyon ve Veri SoyaÄŸacÄ± (Lineage)
### 
Projenin teknik detaylarÄ± ve tablolar arasÄ± iliÅŸkiler **dbt Docs** ile otomatik olarak belgelenmektedir.
-   **[dbt Docs SayfasÄ±](https://sadettinkilic.github.io/dev-training-dataops/)** SaÄŸ alttaki 'Lineage' butonuna tÄ±klayarak akÄ±ÅŸ ÅŸemasÄ±nÄ± inceleyebilirsiniz.
-   **Ä°nteraktif SoyaÄŸacÄ±:** Bronze -> Silver -> Gold katmanlarÄ± arasÄ±ndaki veri akÄ±ÅŸÄ±nÄ± gÃ¶rsel olarak inceleyebilirsiniz.
-   **Veri KataloÄŸu:** Tablo ÅŸemalarÄ±, sÃ¼tun aÃ§Ä±klamalarÄ± ve uygulanan dbt testleri.
  
* * *

## âš™ï¸ Ekstra KonfigÃ¼rasyon NotlarÄ±

Projeyi kendi ortamÄ±nda Ã§alÄ±ÅŸtÄ±rmak isteyenlerin ÅŸu ayarlarÄ± yapmasÄ± gerekir:

-   **GitHub Secrets:** GitHub deponuzun `Settings > Secrets and variables > Actions` kÄ±smÄ±na aÅŸaÄŸÄ±daki deÄŸiÅŸkenleri eklemelisiniz:
    -   `DATABRICKS_HOST`: Databricks instance URL'iniz.
    -   `DATABRICKS_HTTP_PATH`: SQL Warehouse HTTP yolu.
    -   `DATABRICKS_TOKEN`: Key Vault'ta da sakladÄ±ÄŸÄ±nÄ±z eriÅŸim token'Ä±.
        
-   **Environment Variables:** dbt'nin bu secret'lara eriÅŸebilmesi iÃ§in `profiles.yml` dosyasÄ±nda env\_var kullanÄ±mÄ± (Ã–rn: `{{ env_var('DBT_HOST') }}`) yapÄ±landÄ±rÄ±lmÄ±ÅŸtÄ±r.
    

* * *

## ğŸ› ï¸ Projenin Ã–ne Ã‡Ä±kan Yetenekleri

-   **Dinamik Ingestion:** Yeni bir tablo eklemek iÃ§in kod yazmaya gerek kalmadan sadece `param.json` dosyasÄ±nÄ± gÃ¼ncellemek yeterlidir.
    
-   **Secure Secrets:** HiÃ§bir ÅŸifre veya token kodun iÃ§erisinde (hardcoded) bulunmaz; tamamen Azure Key Vault Ã¼zerinden dinamik olarak Ã§ekilir.
    
-   **Medallion Architecture:** Veri kalitesi her katmanda artÄ±rÄ±larak (Raw -> Bronze -> Silver -> Gold) gÃ¼venilir bir "Single Source of Truth" oluÅŸturulur.
    
-   **Serverless Efficiency:** Databricks Serverless SQL Warehouse kullanÄ±larak, sadece sorgu Ã§alÄ±ÅŸtÄ±ÄŸÄ± sÃ¼rece maliyet oluÅŸur ve vCPU kota limitleri aÅŸÄ±lÄ±r.

-   **Automated DataOps:** GitHub Actions ile kod kalitesi (SQLFluff) ve veri tazeliÄŸi (dbt Freshness) otomatik olarak denetlenerek hata payÄ± minimize edilmiÅŸtir.

-   **Separation of Concerns:** Veri dÃ¶nÃ¼ÅŸÃ¼m mantÄ±ÄŸÄ± (dbt) ile veri taÅŸÄ±ma/orkestrasyon mantÄ±ÄŸÄ± (ADF) ayrÄ± depolarda yÃ¶netilerek modÃ¼ler ve bakÄ±mÄ± kolay bir yapÄ± sunulur.
    

* * *

## ğŸ“ˆ Veri Seti HakkÄ±nda

Projede kullanÄ±lan veri seti Kaggle'daki [Paris 2024 Olympic Summer Games](https://www.kaggle.com/datasets/piterfm/paris-2024-olympic-summer-games) setidir. Sporcular, antrenÃ¶rler, madalyalar ve etkinlikler hakkÄ±nda detaylÄ± bilgiler iÃ§erir.

* * *

_Bu dÃ¶kÃ¼man, modern DataOps prensiplerine sadÄ±k kalÄ±narak hazÄ±rlanmÄ±ÅŸtÄ±r._
